{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Report\"\n",
    "subtitle: The Yelpers\n",
    "author: Michael Kim, Cindy Wu, Keaton Olds, Sabrina Kozarovitsky\n",
    "date: 03/14/2023\n",
    "number-sections: true\n",
    "abstract: _The ABSTRACT is to be in fully-justified italicized text at the top of the report, below the author information. The abstract section must summarise the problem statement, the developed model(s), the metric(s) optimized and the recommendations to the stakeholders based on the model. You may also briefly mention any major EDA-based insights that helped develop the model or directly translated into recommendations to the stakeholders. However, the abstract must not be more than 200 words in length_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5ee91",
   "metadata": {},
   "source": [
    "## Length of the report {-}\n",
    "The length of the report must be no more than 15 pages, when printed as PDF. However, there is no requirement on the minimum number of pages.\n",
    "\n",
    "You may put additional stuff as Appendix. You may refer to the Appendix in the main report to support your arguments. However, your appendix is unlikely to be checked while grading, unless the grader deems it necessary. The appendix, references, and information about GitHub and individual contribution will not be included in the page count, and there is no limit on the length of the appendix.\n",
    "\n",
    "**Delete this section from the report, when using this template.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3d9ed",
   "metadata": {},
   "source": [
    "## Code should be put separately in the code template {-}\n",
    "Your report should be in a research-paper like style. If there is something that can only be explained by showing the code, then you may put it, otherwise do not put the code in the report. We will check your code in the code template. \n",
    "\n",
    "**Delete this section from the report, when using this template.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Background / Motivation\n",
    "\n",
    "What motivated you to work on this problem?\n",
    "\n",
    "Mention any background about the problem, if it is required to understand your analysis later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5d5c7",
   "metadata": {},
   "source": [
    "As avid Uber/Lyft riders ourselves, we were naturally drawn to the question of how Ubers/Lyfts are priced -- and which factors contribute most to the pricing of these rides. It comes up a lot in our daily lives, and we wanted to take this as an opportunity to explore this subject more deeply. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1421",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "Describe your problem statement. Articulate your objectives using absolutely no jargon. Interpret the problem as inference and/or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae677f",
   "metadata": {},
   "source": [
    "For this project, we wanted to undestand how certain ride factors (ex. distance of the trip, timestamp, weather, destination, etc.) relate to the price of a ride. Since our response variable (price of the ride) is continuous, this is a regression problem. Our project incoprorated both prediction and inference. One the one hand, we wanted to be able to build a model to predict the price of rides. In addition, we wanted to figure out which factors had the largest impact on price, which has to do with inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b95f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "What data did you use? Provide details about your data. Include links to data if you are using open-access data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3961c3",
   "metadata": {},
   "source": [
    "We used the “Uber & Lyft Cab prices” dataset on Kaggle (https://www.kaggle.com/datasets/ravi72munde/uber-lyft-cab-prices). The dataset includes information on ride prices (collected for a week in November - December 2018) as well as relevant information such as the distance of the trip, timestamp, weather, and destination. We used these factors to help us predict the price of the ride – and determine which factors seem to have a larger effect on price than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255035",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "Who cares? If you are successful, what difference will it make to them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5b15d",
   "metadata": {},
   "source": [
    "We had 3 primary stakeholders:\n",
    "\n",
    "1) Taxi Company and Drivers: they are competitors with Uber/Lyft when it comes to the rideshare market. Through using our model, they can understand rideshare pricing better and adjust accordingly to gain a competitive edge during certain times\n",
    "\n",
    "2) Uber and Lyft Users: people who use Uber/Lyft can use our model to understand when to use rideshare versus other transportation and use this information to plan ahead around high price times to save money\n",
    "\n",
    "3) Uber and Lyft Drivers: as they are direct employees of Uber/Lyft and their salary is directly related to the price of the rides, they are large stakeholders. Through using our model, they can better understand how rides are priced, gauge their expected earnings for a given ride, and have more autonomy over their revenue (ex. choose to take rides during high price times)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "In a tabular form, show the distribution of values of each variable used in the analysis - for both categorical and continuous variables. Distribution of a categorical variable must include the number of missing values, the number of unique values, the frequency of all its levels. If a categorical variable has too many levels, you may just include the counts of the top 3-5 levels. \n",
    "\n",
    "If the tables in this section take too much space, you may put them in the appendix, and just mention any useful insights you obtained from the data quality check that helped you develop the model or helped you realize the necessary data cleaning / preparation.\n",
    "\n",
    "Were there any potentially incorrect values of variables that required cleaning? If yes, how did you clean them? \n",
    "\n",
    "Did you do any data wrangling or data preparation before the data was ready to use for model development? Did you create any new predictors from exisiting predictors? For example, if you have number of transactions and spend in a credit card dataset, you may create spend per transaction for predicting if a customer pays their credit card bill. Mention the steps at a broad level, you may put minor details in the appendix. Only mention the steps that ended up being useful towards developing your final model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd3017c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93296969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>clouds</th>\n",
       "      <th>pressure</th>\n",
       "      <th>rain</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6276.000000</td>\n",
       "      <td>6276.000000</td>\n",
       "      <td>6276.000000</td>\n",
       "      <td>894.000000</td>\n",
       "      <td>6.276000e+03</td>\n",
       "      <td>6276.000000</td>\n",
       "      <td>6276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.090475</td>\n",
       "      <td>0.677777</td>\n",
       "      <td>1008.445209</td>\n",
       "      <td>0.057652</td>\n",
       "      <td>1.543857e+09</td>\n",
       "      <td>0.763985</td>\n",
       "      <td>6.802812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.022055</td>\n",
       "      <td>0.314284</td>\n",
       "      <td>12.870775</td>\n",
       "      <td>0.100758</td>\n",
       "      <td>6.659340e+05</td>\n",
       "      <td>0.127340</td>\n",
       "      <td>3.633466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.620000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>988.250000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.543204e+09</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.077500</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>997.747500</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.543387e+09</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>3.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.130000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>1007.660000</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>1.543514e+09</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>6.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.832500</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1018.480000</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>1.544691e+09</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>9.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.410000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1035.120000</td>\n",
       "      <td>0.780700</td>\n",
       "      <td>1.545159e+09</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>18.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              temp       clouds     pressure        rain    time_stamp  \\\n",
       "count  6276.000000  6276.000000  6276.000000  894.000000  6.276000e+03   \n",
       "mean     39.090475     0.677777  1008.445209    0.057652  1.543857e+09   \n",
       "std       6.022055     0.314284    12.870775    0.100758  6.659340e+05   \n",
       "min      19.620000     0.000000   988.250000    0.000200  1.543204e+09   \n",
       "25%      36.077500     0.440000   997.747500    0.004900  1.543387e+09   \n",
       "50%      40.130000     0.780000  1007.660000    0.014850  1.543514e+09   \n",
       "75%      42.832500     0.970000  1018.480000    0.060925  1.544691e+09   \n",
       "max      55.410000     1.000000  1035.120000    0.780700  1.545159e+09   \n",
       "\n",
       "          humidity         wind  \n",
       "count  6276.000000  6276.000000  \n",
       "mean      0.763985     6.802812  \n",
       "std       0.127340     3.633466  \n",
       "min       0.450000     0.290000  \n",
       "25%       0.670000     3.517500  \n",
       "50%       0.760000     6.570000  \n",
       "75%       0.890000     9.920000  \n",
       "max       0.990000    18.180000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weather data distributions\n",
    "weather = pd.read_csv('weather.csv')\n",
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a6939e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp             0\n",
       "location         0\n",
       "clouds           0\n",
       "pressure         0\n",
       "rain          5382\n",
       "time_stamp       0\n",
       "humidity         0\n",
       "wind             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17950101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37d99024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Back Bay                   523\n",
       "Beacon Hill                523\n",
       "Boston University          523\n",
       "Fenway                     523\n",
       "Financial District         523\n",
       "Haymarket Square           523\n",
       "North End                  523\n",
       "North Station              523\n",
       "Northeastern University    523\n",
       "South Station              523\n",
       "Theatre District           523\n",
       "West End                   523\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e697848b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['distance', 'cab_type', 'time_stamp', 'destination', 'source', 'price',\n",
       "       'surge_multiplier', 'id', 'product_id', 'name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7bf5e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>693071.000000</td>\n",
       "      <td>6.930710e+05</td>\n",
       "      <td>637976.000000</td>\n",
       "      <td>693071.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.189430</td>\n",
       "      <td>1.544046e+12</td>\n",
       "      <td>16.545125</td>\n",
       "      <td>1.013870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.138937</td>\n",
       "      <td>6.891925e+08</td>\n",
       "      <td>9.324359</td>\n",
       "      <td>0.091641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.543204e+12</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.280000</td>\n",
       "      <td>1.543444e+12</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.160000</td>\n",
       "      <td>1.543737e+12</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.920000</td>\n",
       "      <td>1.544828e+12</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.860000</td>\n",
       "      <td>1.545161e+12</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance    time_stamp          price  surge_multiplier\n",
       "count  693071.000000  6.930710e+05  637976.000000     693071.000000\n",
       "mean        2.189430  1.544046e+12      16.545125          1.013870\n",
       "std         1.138937  6.891925e+08       9.324359          0.091641\n",
       "min         0.020000  1.543204e+12       2.500000          1.000000\n",
       "25%         1.280000  1.543444e+12       9.000000          1.000000\n",
       "50%         2.160000  1.543737e+12      13.500000          1.000000\n",
       "75%         2.920000  1.544828e+12      22.500000          1.000000\n",
       "max         7.860000  1.545161e+12      97.500000          3.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rides data distributions\n",
    "rides = pd.read_csv('cab_rides.csv')\n",
    "rides.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccd833fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance                0\n",
       "cab_type                0\n",
       "time_stamp              0\n",
       "destination             0\n",
       "source                  0\n",
       "price               55095\n",
       "surge_multiplier        0\n",
       "id                      0\n",
       "product_id              0\n",
       "name                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39aad26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uber    385663\n",
       "Lyft    307408\n",
       "Name: cab_type, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.cab_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98b3f1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Financial District         58851\n",
       "Theatre District           57798\n",
       "Back Bay                   57780\n",
       "Haymarket Square           57764\n",
       "Boston University          57764\n",
       "Fenway                     57757\n",
       "North End                  57756\n",
       "Northeastern University    57755\n",
       "South Station              57749\n",
       "West End                   57575\n",
       "Beacon Hill                57403\n",
       "North Station              57119\n",
       "Name: destination, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.destination.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc2ef22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Financial District         58857\n",
       "Theatre District           57813\n",
       "Back Bay                   57792\n",
       "Boston University          57764\n",
       "North End                  57763\n",
       "Fenway                     57757\n",
       "Northeastern University    57756\n",
       "South Station              57750\n",
       "Haymarket Square           57736\n",
       "West End                   57562\n",
       "Beacon Hill                57403\n",
       "North Station              57118\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1a1dbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UberXL          55096\n",
       "WAV             55096\n",
       "Black SUV       55096\n",
       "Black           55095\n",
       "Taxi            55095\n",
       "UberX           55094\n",
       "UberPool        55091\n",
       "Lux             51235\n",
       "Lyft            51235\n",
       "Lux Black XL    51235\n",
       "Lyft XL         51235\n",
       "Lux Black       51235\n",
       "Shared          51233\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d950d59",
   "metadata": {},
   "source": [
    "As can be seen by the distributions charts, the only missing values in the weather data set were for rain and the only missing values for the rides data set were for price. We began our cleaning by filling each missing value of rain to be 0. We discovered that there were no data points in the original weather dataset that were 0 and assumed that when there was no rain, no value was reported. Thus, we imputed 0 for all missing values of rain. We then merged the weather and rides datasets to allow us to use weather when predicting price. After doing so, we droped all observations with missing values for price. Since we are predicting price, we need prices for all rides that are used to train our model. \n",
    "\n",
    "Unfortunately, after merging both data sets and dropping observations with no price, we had lost about 690,000 observations. Although this is a significant amount of data lost, our main purpose was to improve upon past prediction models. We wanted to test whether including weather would improve the model's accuracy in predicting price or not. Additionally, because this data was taken over a 1.5 week span, we felt that the 3000 data points left in the data set could still be representative of rides for this time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8d07e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of cleaned data\n",
    "train = pd.read_csv('train_data_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69cdd159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>price</th>\n",
       "      <th>surge_multiplier</th>\n",
       "      <th>temp</th>\n",
       "      <th>clouds</th>\n",
       "      <th>pressure</th>\n",
       "      <th>rain</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "      <th>hour</th>\n",
       "      <th>product_Accessible</th>\n",
       "      <th>product_Basic</th>\n",
       "      <th>product_Lux</th>\n",
       "      <th>product_SUV</th>\n",
       "      <th>product_Sedan</th>\n",
       "      <th>product_Shared</th>\n",
       "      <th>product_XL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.179746</td>\n",
       "      <td>16.823399</td>\n",
       "      <td>1.020439</td>\n",
       "      <td>39.176943</td>\n",
       "      <td>0.686577</td>\n",
       "      <td>998.723226</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.759972</td>\n",
       "      <td>8.035526</td>\n",
       "      <td>9.896899</td>\n",
       "      <td>0.078131</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.075312</td>\n",
       "      <td>0.173983</td>\n",
       "      <td>0.171164</td>\n",
       "      <td>0.159082</td>\n",
       "      <td>0.173580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.138660</td>\n",
       "      <td>9.719118</td>\n",
       "      <td>0.110672</td>\n",
       "      <td>3.403477</td>\n",
       "      <td>0.289554</td>\n",
       "      <td>8.596076</td>\n",
       "      <td>0.095905</td>\n",
       "      <td>0.103227</td>\n",
       "      <td>3.049207</td>\n",
       "      <td>7.297125</td>\n",
       "      <td>0.268432</td>\n",
       "      <td>0.374604</td>\n",
       "      <td>0.263947</td>\n",
       "      <td>0.379171</td>\n",
       "      <td>0.376728</td>\n",
       "      <td>0.365826</td>\n",
       "      <td>0.378825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>990.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.260000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.620000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>991.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>6.470000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.170000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.100000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>996.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.950000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.130000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1004.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.460000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>52.710000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1035.030000</td>\n",
       "      <td>0.780700</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>13.660000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          distance        price  surge_multiplier         temp       clouds  \\\n",
       "count  2483.000000  2483.000000       2483.000000  2483.000000  2483.000000   \n",
       "mean      2.179746    16.823399          1.020439    39.176943     0.686577   \n",
       "std       1.138660     9.719118          0.110672     3.403477     0.289554   \n",
       "min       0.020000     2.500000          1.000000    27.080000     0.000000   \n",
       "25%       1.260000     9.500000          1.000000    37.620000     0.480000   \n",
       "50%       2.170000    14.000000          1.000000    39.100000     0.740000   \n",
       "75%       2.950000    22.500000          1.000000    41.130000     0.970000   \n",
       "max       7.460000    92.000000          2.000000    52.710000     1.000000   \n",
       "\n",
       "          pressure         rain     humidity         wind         hour  \\\n",
       "count  2483.000000  2483.000000  2483.000000  2483.000000  2483.000000   \n",
       "mean    998.723226     0.018685     0.759972     8.035526     9.896899   \n",
       "std       8.596076     0.095905     0.103227     3.049207     7.297125   \n",
       "min     990.640000     0.000000     0.590000     0.480000     0.000000   \n",
       "25%     991.730000     0.000000     0.670000     6.470000     2.000000   \n",
       "50%     996.070000     0.000000     0.720000     9.450000     9.000000   \n",
       "75%    1004.190000     0.000000     0.840000    10.060000    16.000000   \n",
       "max    1035.030000     0.780700     0.990000    13.660000    23.000000   \n",
       "\n",
       "       product_Accessible  product_Basic  product_Lux  product_SUV  \\\n",
       "count         2483.000000    2483.000000  2483.000000  2483.000000   \n",
       "mean             0.078131       0.168747     0.075312     0.173983   \n",
       "std              0.268432       0.374604     0.263947     0.379171   \n",
       "min              0.000000       0.000000     0.000000     0.000000   \n",
       "25%              0.000000       0.000000     0.000000     0.000000   \n",
       "50%              0.000000       0.000000     0.000000     0.000000   \n",
       "75%              0.000000       0.000000     0.000000     0.000000   \n",
       "max              1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       product_Sedan  product_Shared   product_XL  \n",
       "count    2483.000000     2483.000000  2483.000000  \n",
       "mean        0.171164        0.159082     0.173580  \n",
       "std         0.376728        0.365826     0.378825  \n",
       "min         0.000000        0.000000     0.000000  \n",
       "25%         0.000000        0.000000     0.000000  \n",
       "50%         0.000000        0.000000     0.000000  \n",
       "75%         0.000000        0.000000     0.000000  \n",
       "max         1.000000        1.000000     1.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(columns='Unnamed: 0', inplace=True)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd74a9",
   "metadata": {},
   "source": [
    "Put the relevant EDA here (visualizations, tables, etc.) that helped you figure out useful predictors for developing the model(s). Only put the EDA that ended up being useful towards developing your final model(s). \n",
    "\n",
    "List the insights (as bullet points) you got from EDA that ended up being useful towards developing your final model. \n",
    "\n",
    "Again, if there are too many plots / tables, you may put them into appendix, and just mention the insights you got from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "What kind of a model (linear / logistic / other) did you use? What performance metric(s) did you optimize and why?\n",
    "\n",
    "Is there anything unorthodox / new in your approach? \n",
    "\n",
    "What problems did you anticipate? What problems did you encounter? Did the very first model you tried work? \n",
    "\n",
    "Did your problem already have solution(s) (posted on Kaggle or elsewhere). If yes, then how did you build upon those solutions, what did you do differently? Is your model better as compared to those solutions in terms of prediction / inference?\n",
    "\n",
    "**Important: Mention any code repositories (with citations) or other sources that you used, and specifically what changes you made to them for your project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527482f7",
   "metadata": {},
   "source": [
    "The model that we used was an initial single linear regression model, several multiple linear regression models, and finally a lasso regression model. We decided to optimize for the performance metric of RMSE. We did this because we wanted to penalize large errors, since large errors could mean that the customer ends up paying a much higher price than they were expecting if using our model to try and predict the ride price (which could be an undue, large financial strain for them). Therefore, since RMSE properly penalizes these large errors versus other options like MAE (which equally penalizes the error from each observation), we will proceed with utilizing this performance metric to optimize. \n",
    "\n",
    "There was nothing too unorthodox or new in our approach. We decided to start with a base/naive model with what we thought would be the singlemost important predictor (distance), and then we moved onto using all of the predictors in the dataset and the new predictor variables that we created to get new multiple linear regression models. Lastly, we used a lasso regression model considering all of the previously used predictor variables and all possible two-factor interactions as predictors. We decided to use a lasso regression model because doing so could be used for variable selection and could handle the presence of stronger correlated features that existed in the dataset. XXXXXXX *better reason for lasso* XXXXXXX. \n",
    "\n",
    "We anticipated problems when it came to preparing the actual data that allowed us to go about our approach. Specifically, there was only select weather data available for various source and destination locations on specific dates and at specific times. As such, we did in fact encounter this problem when approaching how to best clean and prepare the data itself before going about developing our models. Additionally, we anticipated the problem of multicollinearity when it came to the dummy variables we were going to create on the type of ride and with weather variables. XXXXXX (did we encounter this problem) XXXXXX. Lastly, we wanted to potentially focus on autocorrelation as a possible problem we anticipated and wanted to explore in our data further. The first model that we tried including our created variables and the weather variables only had a slight improvement compared with the base/naive model of all the ride dataset variables. \n",
    "\n",
    "Our problem did already have some solutions posted on Kaggle trying to predict the ride price of Uber/Lyft rides in the same datasets. The one solution of these that we did look at closer decided to approach the problem we encountered with data cleaning/preparation of the weather variables by averaging them out for each source and destination location. After further analysis, we decided that this wouldn't be the best approach, since the average rainfall, cloud coverage, etc. of a given location isn't what we wanted to examine as predictors. Instead, we wanted accurate weather information of an actual given time for a location, which is why we took the steps we did to clean the data and get only the observations that had said weather data for locations at specific times. Further, this solution did not handle the potential issues with multicollinearity or exploration of autocorrelation, which we decided to analyze to build upon this solution. While this model only ended up analyzing R-squared as a performance metric, we do believe that our model was better compared to this solution. Although we cannot compare by RMSE, the solution's model does little to build upon the multiple linear regression model RMSE with all the train and weather predictor variables, and as such, we believe that our lasso regression model does in fact do better when compared to this solution. Additionally, since this solution's model does not handle the possibility of multicollinearity, we believe that our final model is better since we have confirmed that no problematic amounts of multicollinearity exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Explain the steps taken to develop and improve the base model - informative visualizations / addressing modeling assumption violations / variable transformation / interactions / outlier treatment / influential points treatment / addressing over-fitting / addressing multicollinearity / variable selection - stepwise regression, lasso, ridge regression). \n",
    "\n",
    "Did you succeed in achieving your goal, or did you fail? Why?\n",
    "\n",
    "**Put the final model equation**.\n",
    "\n",
    "**Important: This section should be rigorous and thorough. Present detailed information about decision you made, why you made them, and any evidence/experimentation to back them up.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988225bb",
   "metadata": {},
   "source": [
    "The base model was created as a single linear regression model using distance as the only predictor variable and price as the response variable. The resulting RMSE for test data of the model was approximately 9.16 USD. The mean for the price of the cleaned data observations was approximately 16.82 USD and the standard error was approximately 9.86 USD. We decided to make a better base model as a multiple linear regression model using all of the variables from the ride dataset as predictor variables (including distance, source, destination, name, product_id, and cab_type but not surge_multiplier, since this would not be known by the stakeholders at the time of the prediction). We made this decision because we felt that it would be a better representation of comparison for our efforts in improving the base model. The resulting RMSE for test data of the better initial model was approximately 3.63 USD. \n",
    "\n",
    "From this, we examined the predictor variables for potential variable transformations that would be helpful in improving model performance as measured by RMSE for test data. Ultimately, we discovered one possible non-linear relationship between distance and price from a barplot of price against binned distance (with the price increasing slightly more as binned distance increased) that was shown in our code report. As such, we decided it would be prudent to examine whether a log(distance) transformation would improve RMSE if included in a model (which it ended up doing) because of the possible non-linear relationship between price and distance. \n",
    "\n",
    "Next, we decided to examine whether this base multiple linear regression model also violated any modeling assumptions. Specifically, we did so by plotting residuals against fitted values. While this base model did mostly seem to not violate the linearity assumption, there did seem to be a problematic trend as the fitted values increased that residuals tended to stray away from the line of residuals = 0 that could mean it violated the constant variance assumption. As such, we then thought that it would be a good next step to use log(price) as our response variable in the next model we developed. Since this log transformation of price will result in a higher shrinkage of larger values, to some extent we were hoping that this would resolve this issue. \n",
    "\n",
    "We then went about approaching 2-factor interactions and variable selection by creating a lasso regression model. We decided to use a lasso regression model because doing so could handle the presence of stronger correlated features that existed in the dataset. XXXXXXX better reason for lasso XXXXXXX. XXXXXX RESULTS XXXXXXX.\n",
    "\n",
    "We decided to explore whether the presence of outliers and influential points could impact the performance of our lasso regression model. To do so, we used code that XXXXXX EXAMPLE XXXXXX.\n",
    "\n",
    "We examined the possible problem of overfitting in this final lasso regression model that we developed. We used RSE as a test of whether the lasso regression model was in fact overfitting. From our calculations, we found that the RSE was XXXXXXXXX RSE VALUE XXXXXXXXX. Compared with the RMSE value for the model for test data of XXXXXXXXX RMSE VALUE XXXXXXXXXX, the RSE was comparable with that of the RMSE. As such, it means that we can conclude from interpreting these results that the lasso regression model was not overfitting. Should the model return a far lower RSE for the train data than the RMSE for test data, this means that the model parameters were only optimized for traiing data and thus overfit. However, this was not the case because the two values were comparable, as explored in the calculations previously mentioned. \n",
    "\n",
    "MULTICOLLINEARITY?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46343d",
   "metadata": {},
   "source": [
    "## Limitations of the model with regard to inference / prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ead90",
   "metadata": {},
   "source": [
    "If it is inference, will the inference hold for a certain period of time, for a certain subset of population, and / or for certain conditions.\n",
    "\n",
    "If it is prediction, then will it be possible / convenient / expensive for the stakeholders to collect the data relating to the predictors in the model. Using your model, how soon will the stakeholder be able to predict the outcome before the outcome occurs. For example, if the model predicts the number of bikes people will rent in Evanston on a certain day, then how many days before that day will your model be able to make the prediction. This will depend on how soon the data that your model uses becomes available. If you are predicting election results, how many days / weeks / months / years before the election can you predict the results. \n",
    "\n",
    "When will your model become too obsolete to be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d092b",
   "metadata": {},
   "source": [
    "For the inference portion of our project, the inference technically holds for Boston rides, since the dataset only contains Boston rides data observations. However, should our stakeholders want to abstract our analysis on which predictor variables are relatively the most important predictors for ride price, we would need to redevelop our models with data from other cities with the same methods we used. However, to a large extent, the general trends will remain true regardless of the city, such that the overall importance of predictors for ride price can remain more or less useful in other major cities like Boston. Additionally, because this data was from 2018, the importance of predictors might be slightly outdated. However, from research knowing that distance and demand for rides impact the pricing algorithms on Uber/Lyft rides, we still see that this same relative importance of predictors will likely remain similar for rides today. \n",
    "\n",
    "For the prediction portion of our project, the stakeholder will be able to predict the outcome approximately one week out before the outcome occurs up until the actual ride takes place. Since we are using the type of ride, whether the ride was Uber/Lyft, and various weather variables to predict the price of a ride, this information will all become fairly accurate and available a week before one will take the ride (assuming that they have access to weather data through applications/websites that are usually accurate a week prior). If they have the information available about when they are preparing to take the ride and what type of ride they will take, then Uber and Lyft users are able to use our final model to predict what the price of their ride will likely be. Additionally, the same applies for Uber/Lyft drivers and taxi companies and drivers on predicting the likely price of Uber/Lyft rides with the given data. Further, we noted previously that since the data is for Boston rides only, the analysis may only be applicable for Boston (while possibly for other major cities), and the model can be redeveloped with new data for other cities, as well. It is also important to note that due to using a smaller weather dataset than rides dataset, when combined, we lost many data points. Our final data set had very limited data points for Friday, Saturday, and Sunday. Thus, our model may be less accurate for predicting prices on the weekends when demand could be different. However, if more data is collected and the same process of model creation is followed, predictions will become more accurate in the case that our limited weekend data is not representative of all rides on weekends. \n",
    "\n",
    "The model will likely become too obsolete to be useful for prediction in coming years as Uber/Lyft change their pricing algorithm. However, approaching the development of models in a similar way to how we did with the new data can be done to make it more relevant. However, although the relative importance may shift slightly compared to other predictors, largely the inference part of our project won't become too obsolete for quite some years, given that, for instance, distance and the type of ride will remain largely important in Uber/Lyft's pricing algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026cb7",
   "metadata": {},
   "source": [
    "## Other sections *(optional)*\n",
    "\n",
    "You are welcome to introduce additional sections or subsections, if required, to address any specific aspects of your project in detail. For example, you may briefly discuss potential future work that the research community could focus on to make further progress in the direction of your project's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "What conclusions do you draw based on your model? If it is inference you may draw conclusions based on the coefficients, statistical significance of predictors / interactions, etc. If it is prediction, you may draw conclusions based on prediction accuracy, or other performance metrics.\n",
    "\n",
    "How do you use those conclusions to come up with meaningful recommendations for stakeholders? The recommendations must be action-items for stakeholders that they can directly implement without any further analysis. Be as precise as possible. The stakeholder(s) are depending on you to come up with practically implementable recommendations, instead of having to think for themselves.\n",
    "\n",
    "If your recommendations are not practically implementable by stakeholders, how will they help them? Is there some additional data / analysis / domain expertise you need to do to make the recommendations implementable? \n",
    "\n",
    "Do the stakeholder(s) need to be aware about some limitations of your model? Is your model only good for one-time use, or is it possible to update your model at a certain frequency (based on recent data) to keep using it in the future? If it can be used in the future, then for how far into the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ad552",
   "metadata": {},
   "source": [
    "Based on our model, distance and product type are the most important factors that determine the price of a ride, while time of day and source/destination had little impact. Additionally, our model can accurately predict the price of a ride in Boston with an RMSE of $2.22. We recommned that Uber and Lyft riders should walk (if possible) to decrease ride distances and check both Uber and Lyft ride offerings to maximize savings. These would be more impactful than trying to plan a schedule of taking rides at certain times from certain places as those minimally impact ride price. For rideshare drivers, we recommend that they become drivers for both Uber and Lyft, so they can compare the prices of each at different times to maximize revenue. Finally, we recommend Taxi drivers who want to make competitive pricing can use our model to accurately predict the competition ride price to price lower and attract more customers with their better rates. This would be specifically useful to adjust their rates based on distance, as they could see how their prices for a ride of a certain distance compare to that of rideshare companies. One limitation of the model is that our data only considers rides in Boston for a 3 week period. Ride prices could fluctuate marginally in different places during different times of the year, resulting in a potential increase in the RMSE of our model. However, we can reasonably anticipate the general trends of our model to stay consistent regardless of urban area or time of year. This model would benefit from being updated yearly to account for inflation, however. A second limitation of the model is that it wasn't able to quite capture trends in demand for rides using weather, location, and time of day. From our personal experience, we know that demand surges can be hyper-localized: rideshare prices around a stadium/arena after a concert or sporting event tend to skyrocket for example. Surges like these are difficult to predict with a model as they require ultra-specific data about different locations at different times. It would be beneficial to the stakeholders if a model to predict this demand could be developed, but for now riders, drivers, and Taxi should be aware of events near their area and use their intuition regarding influences of demand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44497c",
   "metadata": {},
   "source": [
    "## GitHub and individual contribution {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b3f66",
   "metadata": {},
   "source": [
    "Put the **Github link** for the project repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb72d9e",
   "metadata": {},
   "source": [
    "https://github.com/cndywu/TheYelpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca45613",
   "metadata": {},
   "source": [
    "Add details of each team member's contribution in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505da5c",
   "metadata": {},
   "source": [
    "<html>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "th {\n",
    "  text-align: left;\n",
    "}\n",
    "    \n",
    "\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<h2>Individual contribution</h2>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <colgroup>\n",
    "       <col span=\"1\" style=\"width: 15%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 50%;\">\n",
    "       <col span=\"1\" style=\"width: 15%;\"> \n",
    "    </colgroup>\n",
    "  <tr>\n",
    "    <th>Team member</th>\n",
    "    <th>Contributed aspects</th>\n",
    "    <th>Details</th>\n",
    "    <th>Number of GitHub commits</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Michael Kim</td>\n",
    "    <td>Model Approach Creation, Model Development, and Importance of Predictors (Inference)</td>\n",
    "    <td>Created the base, intermediate, and final models; examined transformations, inference, model assumptions, overfitting, and multicollinearity; did analysis using magnitude of coefficients to find relative importance of predictor variables.</td>\n",
    "    <td>XXXXXXX TBD XXXXXXXX</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Cindy Wu</td>\n",
    "    <td>XXXX</td>\n",
    "    <td>XXXX</td>\n",
    "    <td>XXXXXX TBD XXXXX</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Keaton Olds</td>\n",
    "    <td>Outlier and influential points treatment</td>\n",
    "    <td>Identified outliers/influential points and analayzed their effect on the model.</td>\n",
    "    <td>XXXXXXX TBD XXXXXXX</td>    \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Sabrina Kozarovitsky</td>\n",
    "    <td>Hour and day exploratory data analysis, combined weather/hour/day exploratory data analysis </td>\n",
    "    <td>Cleaned and prepared data for EDA use, analyzed effects of busier hours on demand for rides, attempted to analyze demand by day but data was too limited, combined Cindy's weather EDA findings to further analyze hours</td>\n",
    "    <td>XXXXXXX TBD XXXXXXX</td>    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bbd23",
   "metadata": {},
   "source": [
    "List the **challenges** you faced when collaborating with the team on GitHub. Are you comfortable using GitHub? \n",
    "Do you feel GitHuB made collaboration easier? If not, then why? *(Individual team members can put their opinion separately, if different from the rest of the team)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84cb6e",
   "metadata": {},
   "source": [
    "Our team faced challenges getting acclimated to utilizing GitHub in collaborating, since this was many of our first times using the platform. We found ourselves much more comfortable using GitHub as the project progressed, however, we still feel that there are aspects to the platform that we are unfamiliar with (branches, pull requests, etc.). We think that GitHub did make it easier in having a centralized place to do all of our collaboration; however, we felt that it was tedious to remind each other when we were committing to avoid overwriting other contributions from commits and pushes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1cafe",
   "metadata": {},
   "source": [
    "## References {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb1aad",
   "metadata": {},
   "source": [
    "List and number all bibliographical references. When referenced in the text, enclose the citation number in square brackets, for example [1].\n",
    "\n",
    "[1] Authors. The frobnicatable foo filter, 2014. Face and Gesture submission ID 324. Supplied as additional material\n",
    "fg324.pdf. 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831751c",
   "metadata": {},
   "source": [
    "## Appendix {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d374d",
   "metadata": {},
   "source": [
    "You may put additional stuff here as Appendix. You may refer to the Appendix in the main report to support your arguments. However, the appendix section is unlikely to be checked while grading, unless the grader deems it necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
